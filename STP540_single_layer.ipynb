{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamharkirat/STP-540-Computational-Statistics/blob/main/STP540_single_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY8n6GCOuEDs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbGyFMbqydxY"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMlaro9w01MP"
      },
      "outputs": [],
      "source": [
        "data = np.genfromtxt('/content/data_banknote_authentication.txt', delimiter = ',')\n",
        "X = data[:,:4]\n",
        "y = data[:, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4ddZa6j01rS"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2, c=y, cmap='viridis')\n",
        "plt.xlabel('variance of wavelet')\n",
        "plt.ylabel('skewness of wavelet');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmmlezsc1W6k"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = X_train.T\n",
        "y_train = y_train.reshape(1, y_train.shape[0])\n",
        "X_test = X_test.T\n",
        "y_test = y_test.reshape(1, y_test.shape[0])\n",
        "print('Train X Shape: ', X_train.shape)\n",
        "print('Train Y Shape: ', y_train.shape)\n",
        "print('I have m = %d training examples!' % (X_train.shape[1]))\n",
        "\n",
        "print('Test X Shape: ', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5y8bexu11h0"
      },
      "source": [
        "# Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vebqr75f2RtN"
      },
      "outputs": [],
      "source": [
        "def define_structure(X, Y):\n",
        "    input_unit = X.shape[0] # size of input layer\n",
        "    hidden_unit = 4 #hidden layer of size 4\n",
        "    output_unit = Y.shape[0] # size of output layer\n",
        "    return (input_unit, hidden_unit, output_unit)\n",
        "\n",
        "(input_unit, hidden_unit, output_unit) = define_structure(X_train, y_train)\n",
        "print(\"The size of the input layer is: \" + str(input_unit))\n",
        "print(\"The size of the hidden layer is: \" + str(hidden_unit))\n",
        "print(\"The size of the output layer is: \" + str(output_unit))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWeTdjST2idI"
      },
      "source": [
        "## Initialize Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfAokiiZ2nTS"
      },
      "outputs": [],
      "source": [
        "def parameters_initialization(input_unit, hidden_unit, output_unit):\n",
        "    np.random.seed(2) \n",
        "    W1 = np.random.randn(hidden_unit, input_unit)*0.01\n",
        "    b1 = np.zeros((hidden_unit, 1))\n",
        "    W2 = np.random.randn(output_unit, hidden_unit)*0.01\n",
        "    b2 = np.zeros((output_unit, 1))\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV3pzKb2npj"
      },
      "source": [
        "## Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvREKgP-3F3w"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "    \n",
        "def forward_propagation(X, parameters):\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXEXfnMU3Kei"
      },
      "source": [
        "## Compute Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sQlqBFs3PbJ"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_cost(A2, Y, parameters):\n",
        "    # number of training example\n",
        "    m = Y.shape[1] \n",
        "    # Compute the cross-entropy cost\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))\n",
        "    cost = - np.sum(logprobs) / m\n",
        "    cost = float(np.squeeze(cost))\n",
        "                                    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLSVkOt83R4e"
      },
      "source": [
        "## Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN0hrQoj3WI8"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    #number of training example\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "   \n",
        "    dZ2 = A2-Y\n",
        "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "    dW1 = (1/m) * np.dot(dZ1, X.T) \n",
        "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
        "    \n",
        "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n",
        "    \n",
        "    return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRVLaqJG3YYF"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlR64Db23cEb"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(parameters, grads, learning_rate = 0.01):\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "   \n",
        "    dW1 = grads['dW1']\n",
        "    db1 = grads['db1']\n",
        "    dW2 = grads['dW2']\n",
        "    db2 = grads['db2']\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    \n",
        "    parameters = {\"W1\": W1, \"b1\": b1,\"W2\": W2,\"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD_FcakS3e3J"
      },
      "source": [
        "## Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve3kIIly3i23"
      },
      "outputs": [],
      "source": [
        "def neural_network_model(X, Y, hidden_unit, num_iterations = 1000):\n",
        "    np.random.seed(3)\n",
        "    input_unit = define_structure(X, Y)[0]\n",
        "    output_unit = define_structure(X, Y)[2]\n",
        "    \n",
        "    parameters = parameters_initialization(input_unit, hidden_unit, output_unit)\n",
        "   \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    for i in range(0, num_iterations):\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        cost = cross_entropy_cost(A2, Y, parameters)\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "        parameters = gradient_descent(parameters, grads)\n",
        "        if i % 5 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    return parameters\n",
        "    \n",
        "parameters = neural_network_model(X_train, y_train, 4, num_iterations=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pkQKFR03mqI"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZvpetHI4CJP"
      },
      "outputs": [],
      "source": [
        "def prediction(parameters, X):\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = np.round(A2)\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9XZoA2Z4Cek"
      },
      "outputs": [],
      "source": [
        "predictions = prediction(parameters, X_train)\n",
        "print ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')\n",
        "predictions = prediction(parameters, X_test)\n",
        "print ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWIVmTU4Fiw"
      },
      "source": [
        "# Playing Around with Different Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frZNwjng897-"
      },
      "source": [
        "## Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czQ2DLkz9MdI"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    \n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tanh(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    \n",
        "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
        "    \n",
        "    return A2, cache\n",
        "\n",
        "def compute_cost(A2, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -(1/m) * np.sum(Y*np.log(A2) + (1-Y)*np.log(1-A2))\n",
        "    cost = np.squeeze(cost)\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB5Hzajb9bBG"
      },
      "outputs": [],
      "source": [
        "def neural_network_model(X, Y, n_h, num_iterations, learning_rate):\n",
        "    n_x = X.shape[0]\n",
        "    n_y = Y.shape[0]\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    m = X.shape[1]\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        shuffled_indices = np.random.permutation(m)\n",
        "        \n",
        "        for j in shuffled_indices:\n",
        "            Xj = X[:, j].reshape(-1, 1)\n",
        "            Yj = Y[:, j].reshape(-1, 1)\n",
        "            \n",
        "            A2, cache = forward_propagation(Xj, parameters)\n",
        "            \n",
        "            cost = compute_cost(A2, Yj)\n",
        "            \n",
        "            dZ2 = A2 - Yj\n",
        "            dW2 = np.dot(dZ2, cache[\"A1\"].T)\n",
        "            db2 = dZ2\n",
        "            \n",
        "            dZ1 = np.multiply(np.dot(parameters[\"W2\"].T, dZ2), 1 - np.power(cache[\"A1\"], 2))\n",
        "            dW1 = np.dot(dZ1, Xj.T)\n",
        "            db1 = dZ1\n",
        "            \n",
        "            parameters[\"W1\"] -= learning_rate * dW1\n",
        "            parameters[\"b1\"] -= learning_rate * db1\n",
        "            parameters[\"W2\"] -= learning_rate * dW2\n",
        "            parameters[\"b2\"] -= learning_rate * db2\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLc1hNV6FyX3"
      },
      "source": [
        "## Experiment with different learning rates on stochastic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjLatlvYGQiU",
        "outputId": "f58c9eed-7049-4766-8768-3b73a604e8c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.696047\n",
            "Cost after iteration 0: 0.696256\n",
            "Cost after iteration 0: 0.696262\n",
            "Cost after iteration 100: 0.034851\n",
            "Cost after iteration 200: 0.014476\n",
            "Cost after iteration 300: 0.027190\n",
            "Cost after iteration 400: 0.004730\n",
            "Cost after iteration 0: 0.697229\n",
            "Cost after iteration 100: 0.041448\n",
            "Cost after iteration 200: 0.060641\n",
            "Cost after iteration 300: 0.031315\n",
            "Cost after iteration 400: 0.005421\n",
            "Cost after iteration 500: 0.004367\n",
            "Cost after iteration 600: 0.003523\n",
            "Cost after iteration 700: 0.001403\n",
            "Cost after iteration 800: 0.002138\n",
            "Cost after iteration 900: 0.001869\n",
            "Cost after iteration 0: 0.645890\n",
            "Cost after iteration 0: 0.714113\n",
            "Cost after iteration 0: 0.644621\n",
            "Cost after iteration 100: 0.001087\n",
            "Cost after iteration 200: 0.000562\n",
            "Cost after iteration 300: 0.000312\n",
            "Cost after iteration 400: 0.000193\n",
            "Cost after iteration 0: 0.730794\n",
            "Cost after iteration 100: 0.001410\n",
            "Cost after iteration 200: 0.001242\n",
            "Cost after iteration 300: 0.000078\n",
            "Cost after iteration 400: 0.000184\n",
            "Cost after iteration 500: 0.000123\n",
            "Cost after iteration 600: 0.000088\n",
            "Cost after iteration 700: 0.000169\n",
            "Cost after iteration 800: 0.001644\n",
            "Cost after iteration 900: 0.000041\n",
            "Cost after iteration 0: 0.050979\n",
            "Cost after iteration 0: 0.061592\n",
            "Cost after iteration 0: 0.039766\n"
          ]
        }
      ],
      "source": [
        "# Define a list of hyperparameters to search over\n",
        "hidden_units = [4, 8, 16, 32]\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "num_iterations = [50, 100, 500, 1000]\n",
        "\n",
        "# Create an empty dataframe to store the results\n",
        "results_df_sgd = pd.DataFrame(columns=['hidden_units', 'learning_rate', 'num_iterations', 'accuracy'])\n",
        "\n",
        "# Loop over all combinations of hyperparameters\n",
        "for hidden_unit in hidden_units:\n",
        "    for learning_rate in learning_rates:\n",
        "        for num_iter in num_iterations:\n",
        "            # Train the model with the current set of hyperparameters\n",
        "            parameters = neural_network_model(X_train, y_train, hidden_unit, num_iterations=num_iter, learning_rate=learning_rate)\n",
        "            \n",
        "            # Make predictions on the training and test sets\n",
        "            train_preds = prediction(parameters, X_train)\n",
        "            test_preds = prediction(parameters, X_test)\n",
        "            \n",
        "            # Compute the accuracies\n",
        "            train_acc = float((np.dot(y_train, train_preds.T) + np.dot(1 - y_train, 1 - train_preds.T))/float(y_train.size)*100)\n",
        "            test_acc = float((np.dot(y_test, test_preds.T) + np.dot(1 - y_test, 1 - test_preds.T))/float(y_test.size)*100)\n",
        "            \n",
        "            # Append the results to the dataframe\n",
        "            results_df_sgd = pd.concat([results_df_sgd, pd.DataFrame({'hidden_units': [hidden_unit], 'learning_rate': [learning_rate], 'num_iterations': [num_iter], 'accuracy': [test_acc]})], ignore_index=True)\n",
        "\n",
        "# Sort the dataframe by accuracy in descending order\n",
        "results_df_sgd = results_df_sgd.sort_values(by=['accuracy'], ascending=False)\n",
        "\n",
        "print(results_df_sgd.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df_sgd"
      ],
      "metadata": {
        "id": "vqZHd872wERH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV-UjVg1W3Dz"
      },
      "source": [
        "# Trying the Model on Simulated Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6OyYBSPXalz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the number of training examples and features\n",
        "m = 1000\n",
        "n = 5\n",
        "\n",
        "# Generate random input features\n",
        "X = np.random.randn(n, m)\n",
        "\n",
        "# Define the true parameters of the model\n",
        "W_true = np.random.randn(n, 1)\n",
        "b_true = np.random.randn(1)\n",
        "\n",
        "# Compute the true labels (y = W*X + b + noise)\n",
        "noise = np.random.randn(1, m)\n",
        "y_true = np.dot(W_true.T, X) + b_true + noise\n",
        "\n",
        "# Define a binary classification problem by thresholding the labels at 0\n",
        "y_true_binary = (y_true > 0).astype(int)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train = X[:, :800]\n",
        "y_train = y_true_binary[:, :800]\n",
        "X_test = X[:, 800:]\n",
        "y_test = y_true_binary[:, 800:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLwLzFtNXqeW"
      },
      "outputs": [],
      "source": [
        "# Define a list of hyperparameters to search over\n",
        "hidden_units = [4, 8, 10, 16, 32]\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "num_iterations = [50, 100, 500, 1000]\n",
        "\n",
        "# Create an empty dataframe to store the results\n",
        "results_df_sgd_sim = pd.DataFrame(columns=['hidden_units', 'learning_rate', 'num_iterations', 'accuracy'])\n",
        "\n",
        "# Loop over all combinations of hyperparameters\n",
        "for hidden_unit in hidden_units:\n",
        "    for learning_rate in learning_rates:\n",
        "        for num_iter in num_iterations:\n",
        "            # Train the model with the current set of hyperparameters\n",
        "            parameters = neural_network_model(X_train, y_train, hidden_unit, num_iterations=num_iter, learning_rate=learning_rate)\n",
        "            \n",
        "            # Make predictions on the training and test sets\n",
        "            train_preds = prediction(parameters, X_train)\n",
        "            test_preds = prediction(parameters, X_test)\n",
        "            \n",
        "            # Compute the accuracies\n",
        "            train_acc = float((np.dot(y_train, train_preds.T) + np.dot(1 - y_train, 1 - train_preds.T))/float(y_train.size)*100)\n",
        "            test_acc = float((np.dot(y_test, test_preds.T) + np.dot(1 - y_test, 1 - test_preds.T))/float(y_test.size)*100)\n",
        "            \n",
        "            # Append the results to the dataframe\n",
        "            results_df_sgd_sim = pd.concat([results_df_sgd_sim, pd.DataFrame({'hidden_units': [hidden_unit], 'learning_rate': [learning_rate], 'num_iterations': [num_iter], 'accuracy': [test_acc]})], ignore_index=True)\n",
        "\n",
        "# Sort the dataframe by accuracy in descending order\n",
        "results_df_sgd_sim = results_df_sgd_sim.sort_values(by=['accuracy'], ascending=False)\n",
        "\n",
        "print(results_df_sgd_sim.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df_sgd_sim"
      ],
      "metadata": {
        "id": "k38C35b_6VUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mhQPvqORJhP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLoLvZJcViuV15o3nsHTbZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}